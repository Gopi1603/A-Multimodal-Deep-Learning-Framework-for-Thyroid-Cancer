{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10919098,"sourceType":"datasetVersion","datasetId":6788237},{"sourceId":11116238,"sourceType":"datasetVersion","datasetId":6931201},{"sourceId":302427,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":258229,"modelId":279468}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision torchaudio","metadata":{"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in c:\\users\\pandu\\anaconda3\\lib\\site-packages (2.6.0)Note: you may need to restart the kernel to use updated packages.\n","\n","Requirement already satisfied: torchvision in c:\\users\\pandu\\anaconda3\\lib\\site-packages (0.21.0)\n","Requirement already satisfied: torchaudio in c:\\users\\pandu\\anaconda3\\lib\\site-packages (2.6.0)\n","Requirement already satisfied: filelock in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n","Requirement already satisfied: networkx in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n","Requirement already satisfied: setuptools in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n","Requirement already satisfied: sympy==1.13.1 in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pandu\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"]}],"execution_count":20},{"cell_type":"code","source":"ls","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":[" Volume in drive C is OS\n"," Volume Serial Number is EA49-7D93\n","\n"," Directory of C:\\Users\\pandu\\Desktop\\projectcs499\\src\\testing\n","\n","27-03-2025  11:22 PM    <DIR>          .\n","27-03-2025  04:17 PM    <DIR>          ..\n","27-03-2025  04:18 PM    <DIR>          .ipynb_checkpoints\n","27-03-2025  11:22 PM            28,739 compute-performance-metrics.ipynb\n","27-03-2025  11:24 PM    <DIR>          test\n","27-03-2025  04:50 PM            75,885 thyroid_clean.csv\n","27-03-2025  04:46 PM            46,110 trained_clinical_model.pth\n","27-03-2025  04:44 PM       163,948,074 trained_imaging_model.pth\n","27-03-2025  04:44 PM       163,940,990 trained_imaging_model_dataset2.pth\n","27-03-2025  04:44 PM       163,934,846 trained_imaging_model_dataset3.pth\n","27-03-2025  04:46 PM         1,607,878 trained_multimodal_model.pth\n","               7 File(s)    493,582,522 bytes\n","               4 Dir(s)  17,175,920,640 bytes free\n"]}],"execution_count":22},{"cell_type":"code","source":"!nvidia-smi","metadata":{"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stderr","output_type":"stream","text":["'nvidia-smi' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"execution_count":3},{"cell_type":"code","source":"from torchvision.models import resnet18, ResNet18_Weights\n\n# Use the new API to load pretrained weights\nresnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Cell: Updated HybridImagingModel Using Pretrained ResNet18\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass HybridImagingModel(nn.Module):\n    def __init__(self, output_dim=512):\n        super(HybridImagingModel, self).__init__()\n        # Load pretrained ResNet18 model\n        resnet = models.resnet18(pretrained=True)\n        # Remove the final FC layer\n        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Output shape: (batch, 512, 1, 1)\n        # New fully-connected layer to get desired output dimension (flattening the 512 features)\n        self.fc = nn.Linear(512, output_dim)\n    \n    def forward(self, x):\n        # x is assumed to be resized to 224x224\n        x = self.feature_extractor(x)  # Shape: (batch, 512, 1, 1)\n        x = x.view(x.size(0), -1)        # Flatten to (batch, 512)\n        x = self.fc(x)                   # Output shape: (batch, output_dim)\n        return x\n\n# For completeness, here are the unchanged ClinicalModel and FusionModule\n\nclass ClinicalModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(ClinicalModel, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, output_dim)\n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\nclass FusionModule(nn.Module):\n    def __init__(self, img_dim, clin_dim, fused_dim, num_classes):\n        super(FusionModule, self).__init__()\n        self.fusion = nn.Linear(img_dim + clin_dim, fused_dim)\n        self.classifier = nn.Linear(fused_dim, num_classes)\n        self.risk_regressor = nn.Linear(fused_dim, 1)\n    \n    def forward(self, img_feat, clin_feat):\n        fused_feat = torch.cat((img_feat, clin_feat), dim=1)\n        fused_feat = self.fusion(fused_feat)\n        class_logits = self.classifier(fused_feat)\n        risk_score = self.risk_regressor(fused_feat)\n        return class_logits, risk_score\n\n# Updated MultimodalModel using the new HybridImagingModel\nclass MultimodalModel(nn.Module):\n    def __init__(self, num_classes, clinical_input_dim):\n        super(MultimodalModel, self).__init__()\n        self.imaging_model = HybridImagingModel(output_dim=512)  # Now using ResNet18 backbone\n        self.clinical_model = ClinicalModel(input_dim=clinical_input_dim, output_dim=32)\n        self.fusion_module = FusionModule(img_dim=512, clin_dim=32, fused_dim=256, num_classes=num_classes)\n    \n    def forward(self, image, clinical_data):\n        img_feat = self.imaging_model(image)\n        clin_feat = self.clinical_model(clinical_data)\n        class_logits, risk_score = self.fusion_module(img_feat, clin_feat)\n        return class_logits, risk_score\n\n# Test the updated HybridImagingModel and MultimodalModel with dummy data\ndummy_image = torch.randn(4, 3, 224, 224)  # New input size: 224x224\ndummy_clinical = torch.randn(4, 10)\nmodel_test = MultimodalModel(num_classes=9, clinical_input_dim=10)\nclass_out, risk_out = model_test(dummy_image, dummy_clinical)\nprint(\"Updated HybridImagingModel Test - Classification Output Shape:\", class_out.shape)  # Expected: [4, 9]\nprint(\"Updated HybridImagingModel Test - Risk Prediction Output Shape:\", risk_out.shape)   # Expected: [4, 1]\n","metadata":{"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\pandu\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","C:\\Users\\pandu\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Updated HybridImagingModel Test - Classification Output Shape: torch.Size([4, 9])\n","Updated HybridImagingModel Test - Risk Prediction Output Shape: torch.Size([4, 1])\n"]}],"execution_count":26},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Updated Hybrid Imaging Model ---\n# Note: We assume that the pretrained weights expect the imaging branch to output 64 features.\nclass HybridImagingModel(nn.Module):\n    def __init__(self):\n        super(HybridImagingModel, self).__init__()\n        # Example dummy CNN architecture; in practice, you may be using a pretrained ResNet.\n        # Here, we simulate an architecture that eventually outputs 64 features.\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),  # [B, 32, 320, 320]\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),  # [B, 32, 160, 160]\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # [B, 64, 80, 80]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1))  # [B, 64, 1, 1]\n        )\n        # Flatten to get 64-dim feature vector\n        self.fc = nn.Flatten()\n    \n    def forward(self, x):\n        x = self.features(x)  # [B, 64, 1, 1]\n        x = self.fc(x)        # [B, 64]\n        return x\n\n# --- Clinical Data Model ---\nclass ClinicalModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(ClinicalModel, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, output_dim)  # output_dim is 32 in our case\n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\n# --- Fusion Module ---\n# Here, we update the imaging feature dimension to 64 (instead of 512)\nclass FusionModule(nn.Module):\n    def __init__(self, img_dim, clin_dim, fused_dim, num_classes):\n        super(FusionModule, self).__init__()\n        # img_dim is now 64, clin_dim remains 32.\n        self.fusion = nn.Linear(img_dim + clin_dim, fused_dim)\n        self.classifier = nn.Linear(fused_dim, num_classes)\n        self.risk_regressor = nn.Linear(fused_dim, 1)  # For risk prediction\n    \n    def forward(self, img_feat, clin_feat):\n        fused_feat = torch.cat((img_feat, clin_feat), dim=1)  # [B, 64+32]\n        fused_feat = self.fusion(fused_feat)                  # [B, fused_dim]\n        class_logits = self.classifier(fused_feat)            # [B, num_classes]\n        risk_score = self.risk_regressor(fused_feat)          # [B, 1]\n        return class_logits, risk_score, fused_feat\n\n# --- Multimodal Model ---\nclass MultimodalModel(nn.Module):\n    def __init__(self, num_classes, clinical_input_dim):\n        super(MultimodalModel, self).__init__()\n        self.imaging_model = HybridImagingModel()   # Outputs 64-dim imaging features\n        self.clinical_model = ClinicalModel(input_dim=clinical_input_dim, output_dim=32)\n        # Update fusion module to expect 64-dim imaging features\n        self.fusion_module = FusionModule(img_dim=64, clin_dim=32, fused_dim=256, num_classes=num_classes)\n    \n    def forward(self, image, clinical_data):\n        img_feat = self.imaging_model(image)         # [B, 64]\n        clin_feat = self.clinical_model(clinical_data) # [B, 32]\n        class_logits, risk_score, fused_feat = self.fusion_module(img_feat, clin_feat)\n        return class_logits, risk_score\n\n# Quick local test using dummy data:\ndummy_img = torch.randn(4, 3, 640, 640)\ndummy_clin = torch.randn(4, 10)  # clinical input dimension = 10\nmodel_test = MultimodalModel(num_classes=9, clinical_input_dim=10)\nout_cls, out_risk = model_test(dummy_img, dummy_clin)\nprint(\"Updated HybridImagingModel Test - Classification Output Shape:\", out_cls.shape)  # Expected: [4, 9]\nprint(\"Updated HybridImagingModel Test - Risk Prediction Output Shape:\", out_risk.shape)   # Expected: [4, 1]\n","metadata":{"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated HybridImagingModel Test - Classification Output Shape: torch.Size([4, 9])\n","Updated HybridImagingModel Test - Risk Prediction Output Shape: torch.Size([4, 1])\n"]}],"execution_count":28},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport torchvision.transforms as T\n\nclass MultimodalDataset(Dataset):\n    \"\"\"\n    Loads preprocessed images, imaging labels, and clinical data.\n    Assumes patient id is the first token in the image filename (separated by underscore).\n    \"\"\"\n    def __init__(self, images_dir, labels_dir, clinical_csv, transform=None):\n        self.images_dir = Path(images_dir)\n        self.labels_dir = Path(labels_dir)\n        self.image_files = list(self.images_dir.glob(\"*.jpg\"))\n        self.transform = transform\n        self.clinical_df = pd.read_csv(clinical_csv)\n        self.clinical_df['id'] = self.clinical_df['id'].astype(str)\n        \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        img = cv2.imread(str(img_path))\n        if img is None:\n            raise RuntimeError(f\"Unable to read image: {img_path}\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n        \n        # Read imaging label from corresponding text file (default to 0 if missing)\n        label_file = self.labels_dir / (img_path.stem + \".txt\")\n        imaging_label = 0\n        if label_file.exists() and label_file.stat().st_size > 0:\n            with open(label_file, \"r\") as f:\n                line = f.readline().strip()\n                if line:\n                    imaging_label = int(line.split()[0])\n        \n        # Extract clinical features based on patient id (assumed first token of filename)\n        patient_id = img_path.stem.split(\"_\")[0]\n        clin_row = self.clinical_df[self.clinical_df[\"id\"] == patient_id]\n        if clin_row.empty:\n            clinical_features = torch.zeros(10, dtype=torch.float32)\n        else:\n            clinical_features = torch.tensor(clin_row.drop(columns=[\"id\", \"mal\"], errors=\"ignore\").values[0],\n                                             dtype=torch.float32)\n        \n        return img, torch.tensor(imaging_label, dtype=torch.long), clinical_features\n\n# Define transforms\ntransform = T.Compose([\n    T.ToPILImage(),\n    T.Resize((640, 640)),\n    T.ToTensor(),\n])\n\n# Update these file paths as per your local structure:\ntest_images_dir = \"test/images\"      # Folder containing test images\ntest_labels_dir = \"test/labels\"        # Folder containing test label files\nclinical_csv = \"thyroid_clean.csv\"     # Path to the clinical CSV file\n\n# Create test dataset and DataLoader (limit sample size for testing if needed)\ntest_dataset = MultimodalDataset(test_images_dir, test_labels_dir, clinical_csv, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\nprint(\"Test dataset loaded with\", len(test_dataset), \"samples.\")\n","metadata":{"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["Test dataset loaded with 50 samples.\n"]}],"execution_count":30},{"cell_type":"code","source":"import time\nimport torch\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Make sure your MultimodalModel and MultimodalDataset are defined (see previous cells)\n\n# Free up GPU memory (if any) and force CPU usage\ntorch.cuda.empty_cache()\ndevice = torch.device(\"cpu\")\nprint(\"Using device:\", device)\n\n# Define paths (adjust these according to your local file structure)\nmodel_path = \"trained_multimodal_model.pth\"\ntest_images_dir = \"test/images\"        # Folder containing test images\ntest_labels_dir = \"test/labels\"          # Folder containing test label files\nclinical_csv = \"thyroid_clean.csv\"       # Path to the clinical CSV file\n\n# Create test dataset (the MultimodalDataset should be defined in a previous cell)\ntest_dataset = MultimodalDataset(\n    images_dir=test_images_dir,\n    labels_dir=test_labels_dir,\n    clinical_csv=clinical_csv,\n    transform=T.Compose([\n        T.ToPILImage(),\n        T.Resize((640, 640)),\n        T.ToTensor(),\n    ])\n)\n\nprint(\"Full test dataset size:\", len(test_dataset), \"samples.\")\n\n# To save time, select a small subset for quick evaluation (e.g., 10 samples)\nsubset_indices = list(range(50))\nsubset_dataset = Subset(test_dataset, subset_indices)\n# ..n num_workers=0 for local CPU systems to avoid potential issues.\ntest_loader = DataLoader(subset_dataset, batch_size=4, shuffle=False, num_workers=0)\nprint(\"Using subset test dataset with\", len(subset_dataset), \"samples.\")\n\n# Load the multimodal model on CPU first, then move it to device\nmodel = MultimodalModel(num_classes=9, clinical_input_dim=10).to(\"cpu\")\n# Load state_dict safely (ignoring mismatches if needed)\npretrained_dict = torch.load(model_path, map_location=\"cpu\")\nmodel_dict = model.state_dict()\nfiltered_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\nmodel_dict.update(filtered_dict)\nmodel.load_state_dict(model_dict, strict=False)\nmodel.to(device)\nmodel.eval()\nprint(\"Multimodal model loaded and set to evaluation mode on\", device)\n\n# Start timing the evaluation\nstart_time = time.time()\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels, clinical_data in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        clinical_data = clinical_data.to(device)\n        \n        class_logits, risk_score = model(images, clinical_data)\n        preds = torch.argmax(class_logits, dim=1)\n        \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nend_time = time.time()\n\n# Compute evaluation metrics\nacc = accuracy_score(all_labels, all_preds)\ncm = confusion_matrix(all_labels, all_preds)\nreport = classification_report(all_labels, all_preds, zero_division=0)\n\nprint(\"\\nEvaluation complete.\")\nprint(\"Test Accuracy: {:.2f}%\".format(acc * 100))\nprint(\"Classification Report:\\n\", report)\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Full test dataset size: 50 samples.\n","Using subset test dataset with 50 samples.\n","Multimodal model loaded and set to evaluation mode on cpu\n","\n","Evaluation complete.\n","Test Accuracy: 80.00%\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.80      1.00      0.89         4\n","           1       0.00      0.00      0.00         1\n","\n","    accuracy                           0.80         5\n","   macro avg       0.40      0.50      0.44         5\n","weighted avg       0.64      0.80      0.71         5\n","\n","Total evaluation time: 18000.24 seconds\n"]}],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport os\n\n# Free up GPU memory if needed\ntorch.cuda.empty_cache()\n\n# 1. Set device to CPU (for testing; adjust if you have enough GPU memory)\ndevice = torch.device(\"cpu\")\nprint(\"Using device:\", device)\n\n# 2. Specify model path (adjust the path to your local file)\nmodel_path = \"trained_multimodal_model.pth\"\nprint(\"Loading model from:\", model_path)\n\n# 3. Load the multimodal model on CPU first\n# (Make sure MultimodalModel and its submodules are defined in previous cells)\nmodel = MultimodalModel(num_classes=9, clinical_input_dim=10).cpu()\n\n# 4. Load the state dict safely with filtering to ignore mismatches\nstate_dict = torch.load(model_path, map_location=\"cpu\")\nmodel_dict = model.state_dict()\nfiltered_state_dict = {k: v for k, v in state_dict.items() \n                       if k in model_dict and model_dict[k].shape == v.shape}\nmodel_dict.update(filtered_state_dict)\nmodel.load_state_dict(model_dict, strict=False)\nmodel.to(device)\nmodel.eval()\nprint(\"Multimodal model loaded and set to evaluation mode on\", device)\n\n# 5. Define test dataset paths (update these according to your local structure)\ntest_images_dir = \"test/images\"      # Folder containing test images\ntest_labels_dir = \"test/labels\"        # Folder containing test label files\nclinical_csv = \"thyroid_clean.csv\"     # Path to the clinical CSV file\n\n# 6. Create the test dataset and DataLoader.\n# (Ensure that the class MultimodalDataset and the 'transform' variable are defined in previous cells)\ntest_dataset = MultimodalDataset(test_images_dir, test_labels_dir, clinical_csv, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\nprint(\"Test dataset loaded with\", len(test_dataset), \"samples.\")\n\n# 7. Evaluate the model on the test dataset\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels, clinical_data in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        clinical_data = clinical_data.to(device)\n        \n        class_logits, risk_score = model(images, clinical_data)\n        preds = torch.argmax(class_logits, dim=1)\n        \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# 8. Compute evaluation metrics\nacc = accuracy_score(all_labels, all_preds)\ncm = confusion_matrix(all_labels, all_preds)\nreport = classification_report(all_labels, all_preds, zero_division=0)\n\nprint(\"Test Accuracy: {:.2f}%\".format(acc * 100))\nprint(\"Confusion Matrix:\\n\", cm)\nprint(\"Classification Report:\\n\", report)\n\n# 9. Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=np.unique(all_labels),\n            yticklabels=np.unique(all_labels))\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}