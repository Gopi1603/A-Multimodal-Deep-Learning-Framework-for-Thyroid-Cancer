{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11112193,"sourceType":"datasetVersion","datasetId":6928092},{"sourceId":11116238,"sourceType":"datasetVersion","datasetId":6931201},{"sourceId":10919098,"sourceType":"datasetVersion","datasetId":6788237},{"sourceId":311012,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":263763,"modelId":284860}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as T\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Hugging Face Transformers for BART summarization\nfrom transformers import BartForConditionalGeneration, BartTokenizer\n\n# Set device to CPU (or GPU if available, but here we'll use CPU as per your environment)\ndevice = torch.device(\"cpu\")\nprint(\"Using device:\", device)\n\n# Free up any cached GPU memory (if needed)\ntorch.cuda.empty_cache()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:20:48.642761Z","iopub.execute_input":"2025-03-31T09:20:48.643085Z","iopub.status.idle":"2025-03-31T09:21:12.595947Z","shell.execute_reply.started":"2025-03-31T09:20:48.643059Z","shell.execute_reply":"2025-03-31T09:21:12.595180Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# ----- Hybrid Imaging Model -----\nclass HybridImagingModel(nn.Module):\n    def __init__(self):\n        super(HybridImagingModel, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n        self.fc = nn.Linear(64, 512)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\n# ----- Clinical Model (MLP) -----\nclass ClinicalModel(nn.Module):\n    def __init__(self, input_dim, output_dim=2):\n        super(ClinicalModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, output_dim)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n# ----- Fusion Module -----\nclass FusionModule(nn.Module):\n    def __init__(self, img_dim=512, clin_dim=2, fused_dim=256, num_classes=9):\n        super(FusionModule, self).__init__()\n        self.img_proj = nn.Linear(img_dim, fused_dim)\n        self.clin_proj = nn.Linear(clin_dim, fused_dim)\n        self.gate_fc = nn.Sequential(\n            nn.Linear(2 * fused_dim, fused_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(fused_dim, fused_dim),\n            nn.Sigmoid()\n        )\n        self.classifier = nn.Linear(fused_dim, num_classes)\n        self.risk_predictor = nn.Linear(fused_dim, 1)\n    \n    def forward(self, f_img, f_clin):\n        img_feat = self.img_proj(f_img)\n        clin_feat = self.clin_proj(f_clin)\n        combined = torch.cat((img_feat, clin_feat), dim=1)\n        gate = self.gate_fc(combined)\n        fused_feature = gate * img_feat + (1 - gate) * clin_feat\n        class_logits = self.classifier(fused_feature)\n        risk_score = self.risk_predictor(fused_feature)\n        return class_logits, risk_score, fused_feature\n\n# ----- Integrated Multimodal Model -----\nclass MultimodalModel(nn.Module):\n    def __init__(self, num_classes, clinical_input_dim):\n        super(MultimodalModel, self).__init__()\n        self.imaging_model = HybridImagingModel()  # Outputs 512-dim imaging features\n        self.clinical_model = ClinicalModel(input_dim=clinical_input_dim, output_dim=2)\n        self.fusion_module = FusionModule(img_dim=512, clin_dim=2, fused_dim=256, num_classes=num_classes)\n    \n    def forward(self, image, clinical_data):\n        img_feat = self.imaging_model(image)         # (batch, 512)\n        clin_feat = self.clinical_model(clinical_data) # (batch, 2)\n        class_logits, risk_score, _ = self.fusion_module(img_feat, clin_feat)\n        return class_logits, risk_score\n\n# Quick test with dummy data:\ndummy_image = torch.randn(4, 3, 640, 640)\ndummy_clinical = torch.randn(4, 10)  # Assume clinical_input_dim = 10\nmodel_test = MultimodalModel(num_classes=9, clinical_input_dim=10)\nclass_out, risk_out = model_test(dummy_image, dummy_clinical)\nprint(\"Multimodal Model Classification Output Shape:\", class_out.shape)  # Expected: [4, 9]\nprint(\"Multimodal Model Risk Prediction Output Shape:\", risk_out.shape)   # Expected: [4, 1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:21:16.811187Z","iopub.execute_input":"2025-03-31T09:21:16.811864Z","iopub.status.idle":"2025-03-31T09:21:16.930211Z","shell.execute_reply.started":"2025-03-31T09:21:16.811832Z","shell.execute_reply":"2025-03-31T09:21:16.929423Z"}},"outputs":[{"name":"stdout","text":"Multimodal Model Classification Output Shape: torch.Size([4, 9])\nMultimodal Model Risk Prediction Output Shape: torch.Size([4, 1])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\n\n# Make sure the following variable 'device' is defined in a previous cell (or define here)\ndevice = torch.device(\"cpu\")  # For local testing on CPU\n\n# Path to your saved multimodal model (.pth file)\nmodel_path = \"/kaggle/input/multi_model/pytorch/default/1/trained_multimodal_model.pth\"\nprint(\"Loading model from:\", model_path)\n\n# Set model parameters based on your training setup:\n# - num_classes: the number of imaging classes (here 9)\n# - clinical_input_dim: the original clinical feature dimension (e.g., 18) that was reduced to 2 by your ClinicalModel\nclinical_input_dim = 18  # Update based on your clinical CSV preprocessing\nnum_classes = 9\n\n# Initialize the model on CPU first to avoid memory spikes\nmodel = MultimodalModel(num_classes=num_classes, clinical_input_dim=clinical_input_dim).to(\"cpu\")\n\n# Load the state dictionary on CPU (using strict=False to ignore minor mismatches)\nstate_dict = torch.load(model_path, map_location=\"cpu\")\nmodel.load_state_dict(state_dict, strict=False)\n\n# Move the model to the target device and set to evaluation mode\nmodel.to(device)\nmodel.eval()\nprint(\"Multimodal model loaded and set to evaluation mode on\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:25:15.235295Z","iopub.execute_input":"2025-03-31T09:25:15.235647Z","iopub.status.idle":"2025-03-31T09:25:16.576109Z","shell.execute_reply.started":"2025-03-31T09:25:15.235620Z","shell.execute_reply":"2025-03-31T09:25:16.575017Z"}},"outputs":[{"name":"stdout","text":"Loading model from: /kaggle/input/multi_model/pytorch/default/1/trained_multimodal_model.pth\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-9-07d2d6d182af>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"Multimodal model loaded and set to evaluation mode on cpu\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# For example, load a new ultrasound image for inference:\ntest_image_path = \"/kaggle/input/dataset3-follicular/Follicular_Variant_Thyroid_CA.v1i.yolov9/test/images/FVPTC100x-digitalzoom-well-developed-PTC-nuclear-features_png.rf.2bfc16e232a7c744345f3e7ccda6aa3b.jpg\"  # Update with your file path\ntest_clinical_csv = \"/kaggle/input/clinical-dataset/thyroid_clean.csv\"  # The CSV file with clinical data\n\n# Load and preprocess the image\nimage = cv2.imread(test_image_path)\nif image is None:\n    raise RuntimeError(f\"Unable to read test image: {test_image_path}\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n# Use the same transforms as during training (adjust size if needed)\ntransform = T.Compose([\n    T.ToPILImage(),\n    T.Resize((640, 640)),\n    T.ToTensor(),\n])\nimage_tensor = transform(image).unsqueeze(0)  # Shape: [1, 3, 640, 640]\n\n# Prepare clinical data input:\n# Option 1: Manually input the clinical data as a dictionary (example values)\nclinical_input = {\n    \"age\": 45,\n    \"gender\": 1,\n    \"FT3\": 4.2,\n    \"FT4\": 13.7,\n    \"TSH\": 1.5,\n    \"TPO\": 0.3,\n    \"TGAb\": 1.2,\n    \"site\": 0,\n    \"echo_pattern\": 0,\n    \"multifocality\": 0,\n    \"size\": 1.2,\n    \"shape\": 0,\n    \"margin\": 1,\n    \"calcification\": 0,\n    \"echo_strength\": 3,\n    \"blood_flow\": 0,\n    \"composition\": 2,\n    \"multilateral\": 0\n}\n# Convert the clinical input into a tensor.\n# Ensure the order of features matches the order used during training (after dropping 'id' and 'mal').\nclinical_features = np.array([clinical_input[key] for key in sorted(clinical_input.keys())], dtype=np.float32)\nclinical_tensor = torch.tensor(clinical_features).unsqueeze(0)  # Shape: [1, clinical_input_dim]\n\nprint(\"Test image and clinical data prepared.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:27:49.651842Z","iopub.execute_input":"2025-03-31T09:27:49.652355Z","iopub.status.idle":"2025-03-31T09:27:49.748710Z","shell.execute_reply.started":"2025-03-31T09:27:49.652319Z","shell.execute_reply":"2025-03-31T09:27:49.747682Z"}},"outputs":[{"name":"stdout","text":"Test image and clinical data prepared.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    # Run the model\n    class_logits, risk_score = model(image_tensor.to(device), clinical_tensor.to(device))\n    predicted_class = torch.argmax(class_logits, dim=1).item()\n    risk_value = risk_score.squeeze().item()\n\nprint(f\"Predicted Class: {predicted_class}\")\nprint(f\"Risk Score: {risk_value:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:27:56.345568Z","iopub.execute_input":"2025-03-31T09:27:56.345852Z","iopub.status.idle":"2025-03-31T09:27:56.374295Z","shell.execute_reply.started":"2025-03-31T09:27:56.345831Z","shell.execute_reply":"2025-03-31T09:27:56.373205Z"}},"outputs":[{"name":"stdout","text":"Predicted Class: 1\nRisk Score: 0.24\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Construct a prompt based on the multimodal model outputs.\n# Map your predicted class to a diagnosis label (for example):\ndiagnosis_map = {\n    0: \"Benign\",\n    1: \"Papillary Thyroid Carcinoma\",\n    2: \"Follicular Thyroid Carcinoma\",\n    3: \"Medullary Thyroid Carcinoma\",\n    4: \"Anaplastic Thyroid Carcinoma\",\n    5: \"Other\",\n    6: \"Undetermined\",\n    7: \"Suspicious\",\n    8: \"Normal\"\n}\ndiagnosis = diagnosis_map.get(predicted_class, \"Unknown\")\n\nprompt = (f\"Based on the ultrasound image analysis and clinical data, \"\n          f\"the model predicts {diagnosis} with a risk score of {risk_value:.2f}. \"\n          f\"Please generate a concise diagnostic report summarizing these findings.\")\n\nprint(\"Generated prompt for summarization:\")\nprint(prompt)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:28:00.192020Z","iopub.execute_input":"2025-03-31T09:28:00.192298Z","iopub.status.idle":"2025-03-31T09:28:00.197833Z","shell.execute_reply.started":"2025-03-31T09:28:00.192277Z","shell.execute_reply":"2025-03-31T09:28:00.196923Z"}},"outputs":[{"name":"stdout","text":"Generated prompt for summarization:\nBased on the ultrasound image analysis and clinical data, the model predicts Papillary Thyroid Carcinoma with a risk score of 0.24. Please generate a concise diagnostic report summarizing these findings.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Load the pretrained BART model and tokenizer for summarization.\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nbart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n\n# Tokenize the prompt.\ninputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n\n# Generate the summary.\nsummary_ids = bart_model.generate(inputs.input_ids, num_beams=4, max_length=150, early_stopping=True)\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nprint(\"Generated Diagnostic Report:\")\nprint(summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:28:11.969750Z","iopub.execute_input":"2025-03-31T09:28:11.970042Z","iopub.status.idle":"2025-03-31T09:28:28.931920Z","shell.execute_reply.started":"2025-03-31T09:28:11.970021Z","shell.execute_reply":"2025-03-31T09:28:28.931026Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca26062a9a5e459090f23e9a9895d79c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe5346e17e3c4630b2d70381355b6ffa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19a81849c3074d7bbd16a01f47bbb81c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbab7baeead04a38bad67b4fb14a1036"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6443b363d3004c88bb7741dc5ca372e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6b9604ea05b41db94408bf5e761939b"}},"metadata":{}},{"name":"stdout","text":"Generated Diagnostic Report:\nBased on the ultrasound image analysis and clinical data, the model predicts Papillary Thyroid Carcinoma with a risk score of 0.24. Please generate a concise diagnostic report summarizing these findings. Based on the Ultrasound image analysis, clinical data and the model, the models predicts a risk of 1 in 100,000.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Format the final output as a JSON-like dictionary.\nfinal_output = {\n    \"diagnosis\": diagnosis,\n    \"risk_score\": risk_value,\n    \"report\": summary\n}\n\nprint(\"Final Output:\")\nprint(final_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:28:45.241719Z","iopub.execute_input":"2025-03-31T09:28:45.242021Z","iopub.status.idle":"2025-03-31T09:28:45.247005Z","shell.execute_reply.started":"2025-03-31T09:28:45.241998Z","shell.execute_reply":"2025-03-31T09:28:45.246141Z"}},"outputs":[{"name":"stdout","text":"Final Output:\n{'diagnosis': 'Papillary Thyroid Carcinoma', 'risk_score': 0.24345403909683228, 'report': 'Based on the ultrasound image analysis and clinical data, the model predicts Papillary Thyroid Carcinoma with a risk score of 0.24. Please generate a concise diagnostic report summarizing these findings. Based on the Ultrasound image analysis, clinical data and the model, the models predicts a risk of 1 in 100,000.'}\n","output_type":"stream"}],"execution_count":15}]}