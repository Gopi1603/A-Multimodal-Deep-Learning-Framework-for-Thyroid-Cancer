{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11116238,"sourceType":"datasetVersion","datasetId":6931201},{"sourceId":295827,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":253267,"modelId":274728},{"sourceId":295852,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":253285,"modelId":274728}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/input/processed-datasets/processed_dataset1/kaggle/working/data/processed/dataset1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:40:14.111242Z","iopub.execute_input":"2025-03-22T04:40:14.111510Z","iopub.status.idle":"2025-03-22T04:40:14.259783Z","shell.execute_reply.started":"2025-03-22T04:40:14.111462Z","shell.execute_reply":"2025-03-22T04:40:14.258536Z"}},"outputs":[{"name":"stdout","text":"test  train  train_aug\tvalid\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNNBranch(nn.Module):\n    \"\"\"\n    CNN branch to extract local features.\n    Input: (batch, 3, 640, 640)\n    Output: (batch, 128) after global average pooling.\n    \"\"\"\n    def __init__(self):\n        super(CNNBranch, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)  # (32, 320, 320)\n        )\n        self.block2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)  # (64, 160, 160)\n        )\n        self.block3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)  # (128, 80, 80)\n        )\n    \n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        return x.view(x.size(0), -1)\n\n# Quick test:\ndummy_input = torch.randn(4, 3, 640, 640)\ncnn_model = CNNBranch()\nprint(\"CNN Branch Output Shape:\", cnn_model(dummy_input).shape)  # Expected: [4, 128]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:40:52.649182Z","iopub.execute_input":"2025-03-22T04:40:52.649388Z","iopub.status.idle":"2025-03-22T04:40:57.824646Z","shell.execute_reply.started":"2025-03-22T04:40:52.649368Z","shell.execute_reply":"2025-03-22T04:40:57.823743Z"}},"outputs":[{"name":"stdout","text":"CNN Branch Output Shape: torch.Size([4, 128])\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ViTBranch(nn.Module):\n    \"\"\"\n    ViT branch to extract global features.\n    Splits the image into patches, embeds them, adds positional encoding,\n    and processes them with Transformer encoder layers.\n    Input: (batch, 3, 640, 640)\n    Output: (batch, 768)\n    \"\"\"\n    def __init__(self, image_size=640, patch_size=16, in_channels=3, embed_dim=768, num_layers=7, num_heads=8):\n        super(ViTBranch, self).__init__()\n        self.patch_size = patch_size\n        num_patches = (image_size // patch_size) ** 2  # 1600 patches for 640x640\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n        # Use batch_first=True to improve performance and debugging\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=0.1, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    \n    def forward(self, x):\n        x = self.proj(x)  # (batch, embed_dim, 40, 40)\n        x = x.flatten(2).transpose(1, 2)  # (batch, 1600, embed_dim)\n        x = x + self.pos_embedding\n        x = self.transformer(x)\n        x = x.mean(dim=1)\n        return x\n\n# Quick test:\nvit_model = ViTBranch()\nprint(\"ViT Branch Output Shape:\", vit_model(dummy_input).shape)  # Expected: [4, 768]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:41:05.815314Z","iopub.execute_input":"2025-03-22T04:41:05.815588Z","iopub.status.idle":"2025-03-22T04:41:20.890104Z","shell.execute_reply.started":"2025-03-22T04:41:05.815567Z","shell.execute_reply":"2025-03-22T04:41:20.889191Z"}},"outputs":[{"name":"stdout","text":"ViT Branch Output Shape: torch.Size([4, 768])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass HybridImagingModel(nn.Module):\n    \"\"\"\n    Hybrid Imaging Model combining the CNN and ViT branches.\n    Uses simple concatenation of CNN (128-dim) and ViT (768-dim) outputs,\n    then projects to a 512-dim feature vector.\n    \"\"\"\n    def __init__(self):\n        super(HybridImagingModel, self).__init__()\n        self.cnn_branch = CNNBranch()  # Defined in Cell 1\n        self.vit_branch = ViTBranch()    # Defined in Cell 2\n        self.fc = nn.Linear(128 + 768, 512)\n    \n    def forward(self, x):\n        cnn_features = self.cnn_branch(x)\n        vit_features = self.vit_branch(x)\n        fused_features = torch.cat((cnn_features, vit_features), dim=1)\n        out = self.fc(fused_features)\n        return out\n\n# Quick test:\nhybrid_model = HybridImagingModel()\nprint(\"Hybrid Model Output Shape:\", hybrid_model(dummy_input).shape)  # Expected: [4, 512]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:41:27.764065Z","iopub.execute_input":"2025-03-22T04:41:27.764382Z","iopub.status.idle":"2025-03-22T04:41:42.205579Z","shell.execute_reply.started":"2025-03-22T04:41:27.764353Z","shell.execute_reply":"2025-03-22T04:41:42.204912Z"}},"outputs":[{"name":"stdout","text":"Hybrid Model Output Shape: torch.Size([4, 512])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%env CUDA_LAUNCH_BLOCKING=1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:41:48.777307Z","iopub.execute_input":"2025-03-22T04:41:48.777610Z","iopub.status.idle":"2025-03-22T04:41:48.782338Z","shell.execute_reply.started":"2025-03-22T04:41:48.777574Z","shell.execute_reply":"2025-03-22T04:41:48.781627Z"}},"outputs":[{"name":"stdout","text":"env: CUDA_LAUNCH_BLOCKING=1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport cv2\nfrom pathlib import Path\n\n# --- Define Utility and Dataset classes (reuse from above) ---\n\ndef get_unique_labels(labels_dir):\n    labels_dir = Path(labels_dir)\n    unique_labels = set()\n    for label_file in labels_dir.glob(\"*.txt\"):\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                try:\n                    unique_labels.add(int(line.split()[0]))\n                except Exception as e:\n                    print(f\"Error reading label from {label_file}: {e}\")\n    return unique_labels\n\nclass ProcessedImagingDataset(Dataset):\n    \"\"\"\n    Custom dataset for preprocessed images and labels.\n    Filters out images with missing or empty label files.\n    \"\"\"\n    def __init__(self, images_dir, labels_dir, transform=None):\n        self.images_dir = Path(images_dir)\n        self.labels_dir = Path(labels_dir)\n        all_images = list(self.images_dir.glob(\"*.jpg\"))\n        # Filter images that have a corresponding non-empty label file\n        self.image_files = []\n        for img in all_images:\n            label_file = self.labels_dir / (img.stem + \".txt\")\n            if label_file.exists() and label_file.stat().st_size > 0:\n                self.image_files.append(img)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        img = cv2.imread(str(img_path))\n        if img is None:\n            raise RuntimeError(f\"Unable to read image: {img_path}\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n        \n        label_file = self.labels_dir / (img_path.stem + \".txt\")\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                label = int(line.split()[0])\n            else:\n                raise ValueError(f\"No label found in {label_file}\")\n        \n        label = torch.tensor(label, dtype=torch.long)\n        if label.item() < 0 or label.item() > 8:\n            raise ValueError(f\"Label {label.item()} from {label_file} is out of expected range [0, 8].\")\n        return img, label\n\n# ImagingClassifier definition (using HybridImagingModel from Cell 3)\nclass ImagingClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(ImagingClassifier, self).__init__()\n        self.hybrid_model = HybridImagingModel()  # Defined in Cell 3\n        self.classifier = nn.Linear(512, num_classes)\n    \n    def forward(self, x):\n        features = self.hybrid_model(x)\n        logits = self.classifier(features)\n        return logits\n\ndef train_model(model, dataloader, criterion, optimizer, device, num_epochs=20):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += torch.sum(preds == labels)\n            total += labels.size(0)\n        \n        epoch_loss = running_loss / total\n        epoch_acc = correct.double() / total\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n    return model\n\n# --- Main Training Loop ---\nif __name__ == \"__main__\":\n    # Set paths for Dataset1 processed data.\n    images_dir = \"/kaggle/input/processed-datasets/processed_dataset1/kaggle/working/data/processed/dataset1/train/images\"\n    labels_dir = \"/kaggle/input/processed-datasets/processed_dataset1/kaggle/working/data/processed/dataset1/train/labels\"\n    \n    unique_labels = get_unique_labels(labels_dir)\n    print(\"Unique labels in training set:\", unique_labels)\n    num_classes = len(unique_labels)\n    \n    transform = T.Compose([\n        T.ToPILImage(),\n        T.Resize((640, 640)),\n        T.ToTensor(),\n    ])\n    \n    train_dataset = ProcessedImagingDataset(images_dir, labels_dir, transform=transform)\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n    \n    # Test forward pass on CPU.\n    dummy_img, dummy_label = train_dataset[0]\n    dummy_img = dummy_img.unsqueeze(0)\n    model_cpu = ImagingClassifier(num_classes=num_classes)\n    try:\n        test_out = model_cpu(dummy_img)\n        print(\"Forward pass on CPU successful, output shape:\", test_out.shape)\n    except Exception as e:\n        print(\"Error during forward pass on CPU:\", e)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = ImagingClassifier(num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    \n    print(\"Starting training of the Imaging Module on Dataset1...\")\n    trained_model = train_model(model, train_loader, criterion, optimizer, device, num_epochs=20)\n    \n    torch.save(trained_model.state_dict(), \"trained_imaging_model.pth\")\n    print(\"Training completed and model saved as trained_imaging_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T01:17:50.720459Z","iopub.execute_input":"2025-03-22T01:17:50.720784Z","iopub.status.idle":"2025-03-22T02:07:05.410359Z","shell.execute_reply.started":"2025-03-22T01:17:50.720757Z","shell.execute_reply":"2025-03-22T02:07:05.409241Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport cv2\nfrom pathlib import Path\n\n# -------------------------------\n# Utility Function: Get Unique Labels\n# -------------------------------\ndef get_unique_labels(labels_dir):\n    labels_dir = Path(labels_dir)\n    unique_labels = set()\n    for label_file in labels_dir.glob(\"*.txt\"):\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                try:\n                    unique_labels.add(int(line.split()[0]))\n                except Exception as e:\n                    print(f\"Error reading label from {label_file}: {e}\")\n    return unique_labels\n\n# -------------------------------\n# Custom Dataset for Processed Imaging Data\n# -------------------------------\nclass ProcessedImagingDataset(Dataset):\n    \"\"\"\n    Custom dataset for loading preprocessed images and corresponding labels.\n    Filters out images with missing or empty label files.\n    \"\"\"\n    def __init__(self, images_dir, labels_dir, transform=None):\n        self.images_dir = Path(images_dir)\n        self.labels_dir = Path(labels_dir)\n        all_images = list(self.images_dir.glob(\"*.jpg\"))\n        # Only keep images with non-empty label files.\n        self.image_files = [img for img in all_images \n                            if (self.labels_dir / (img.stem + \".txt\")).exists() \n                            and (self.labels_dir / (img.stem + \".txt\")).stat().st_size > 0]\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        img = cv2.imread(str(img_path))\n        if img is None:\n            raise RuntimeError(f\"Unable to read image: {img_path}\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n        \n        label_file = self.labels_dir / (img_path.stem + \".txt\")\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                label = int(line.split()[0])\n            else:\n                raise ValueError(f\"No label found in {label_file}\")\n        label = torch.tensor(label, dtype=torch.long)\n        if label.item() < 0 or label.item() > 8:\n            raise ValueError(f\"Label {label.item()} from {label_file} is out of expected range [0, 8].\")\n        return img, label\n\n# -------------------------------\n# ImagingClassifier Definition\n# -------------------------------\n# Assumes HybridImagingModel is already defined in previous cells.\nclass ImagingClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(ImagingClassifier, self).__init__()\n        self.hybrid_model = HybridImagingModel()  # Defined in earlier cells\n        self.classifier = nn.Linear(512, num_classes)\n    \n    def forward(self, x):\n        features = self.hybrid_model(x)\n        logits = self.classifier(features)\n        return logits\n\n# -------------------------------\n# Training Function\n# -------------------------------\ndef train_model(model, dataloader, criterion, optimizer, device, num_epochs=30):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += torch.sum(preds == labels)\n            total += labels.size(0)\n        \n        epoch_loss = running_loss / total\n        epoch_acc = correct.double() / total\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n    return model\n\n# -------------------------------\n# Main Training Loop for Dataset2 with Fine-Tuning\n# -------------------------------\nif __name__ == \"__main__\":\n    # Set paths for processed Dataset2\n    images_dir = \"/kaggle/input/processed-datasets/processed_dataset2/kaggle/working/data/processed/dataset2/train/images\"\n    labels_dir = \"/kaggle/input/processed-datasets/processed_dataset2/kaggle/working/data/processed/dataset2/train/labels\"\n    \n    unique_labels = get_unique_labels(labels_dir)\n    print(\"Unique labels in training set (Dataset2):\", unique_labels)\n    num_classes = len(unique_labels)\n    \n    transform = T.Compose([\n        T.ToPILImage(),\n        T.Resize((640, 640)),\n        T.ToTensor(),\n    ])\n    \n    train_dataset = ProcessedImagingDataset(images_dir, labels_dir, transform=transform)\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n    \n    # Test forward pass on CPU.\n    dummy_img, dummy_label = train_dataset[0]\n    dummy_img = dummy_img.unsqueeze(0)\n    model_cpu = ImagingClassifier(num_classes=num_classes)\n    try:\n        test_out = model_cpu(dummy_img)\n        print(\"Forward pass on CPU successful, output shape:\", test_out.shape)\n    except Exception as e:\n        print(\"Error during forward pass on CPU:\", e)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = ImagingClassifier(num_classes=num_classes).to(device)\n    \n    # Optional: Fine-tuning - load pretrained weights from Dataset1, excluding classifier layer.\n    pretrained_path = \"/kaggle/input/fine-tuning/pytorch/default/1/trained_imaging_model.pth\"\n    if os.path.exists(pretrained_path):\n        state_dict = torch.load(pretrained_path, map_location=device)\n        # Remove classifier keys from the pretrained state dict.\n        filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"classifier\")}\n        model.load_state_dict(filtered_state_dict, strict=False)\n        print(\"Loaded pretrained weights from Dataset1 (excluding classifier) for fine-tuning.\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n    \n    print(\"Starting fine-tuning training on Dataset2...\")\n    trained_model = train_model(model, train_loader, criterion, optimizer, device, num_epochs=30)\n    \n    torch.save(trained_model.state_dict(), \"trained_imaging_model_dataset2.pth\")\n    print(\"Training completed and model saved as trained_imaging_model_dataset2.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:59:44.969188Z","iopub.execute_input":"2025-03-22T04:59:44.969478Z","iopub.status.idle":"2025-03-22T05:14:03.858482Z","shell.execute_reply.started":"2025-03-22T04:59:44.969455Z","shell.execute_reply":"2025-03-22T05:14:03.857210Z"}},"outputs":[{"name":"stdout","text":"Unique labels in training set (Dataset2): {0, 1, 2, 3, 4}\nForward pass on CPU successful, output shape: torch.Size([1, 5])\nLoaded pretrained weights from Dataset1 (excluding classifier) for fine-tuning.\nStarting fine-tuning training on Dataset2...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-7-f0a28212553e>:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(pretrained_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 - Loss: 1.6576 - Accuracy: 0.3196\nEpoch 2/30 - Loss: 1.3459 - Accuracy: 0.4948\nEpoch 3/30 - Loss: 1.3160 - Accuracy: 0.5464\nEpoch 4/30 - Loss: 1.1456 - Accuracy: 0.5979\nEpoch 5/30 - Loss: 1.1816 - Accuracy: 0.5155\nEpoch 6/30 - Loss: 1.1437 - Accuracy: 0.5258\nEpoch 7/30 - Loss: 1.0999 - Accuracy: 0.5361\nEpoch 8/30 - Loss: 1.1172 - Accuracy: 0.5670\nEpoch 9/30 - Loss: 1.1534 - Accuracy: 0.5258\nEpoch 10/30 - Loss: 0.9880 - Accuracy: 0.5876\nEpoch 11/30 - Loss: 1.0780 - Accuracy: 0.6186\nEpoch 12/30 - Loss: 1.0064 - Accuracy: 0.6186\nEpoch 13/30 - Loss: 1.0196 - Accuracy: 0.6907\nEpoch 14/30 - Loss: 0.9854 - Accuracy: 0.6495\nEpoch 15/30 - Loss: 0.9998 - Accuracy: 0.6289\nEpoch 16/30 - Loss: 1.0522 - Accuracy: 0.5567\nEpoch 17/30 - Loss: 1.0978 - Accuracy: 0.5567\nEpoch 18/30 - Loss: 0.9626 - Accuracy: 0.6392\nEpoch 19/30 - Loss: 0.9737 - Accuracy: 0.6701\nEpoch 20/30 - Loss: 1.0405 - Accuracy: 0.5876\nEpoch 21/30 - Loss: 0.9914 - Accuracy: 0.5773\nEpoch 22/30 - Loss: 0.8878 - Accuracy: 0.6598\nEpoch 23/30 - Loss: 1.0295 - Accuracy: 0.5567\nEpoch 24/30 - Loss: 1.0372 - Accuracy: 0.5876\nEpoch 25/30 - Loss: 0.9555 - Accuracy: 0.6289\nEpoch 26/30 - Loss: 0.8908 - Accuracy: 0.6082\nEpoch 27/30 - Loss: 0.8829 - Accuracy: 0.6495\nEpoch 28/30 - Loss: 0.8403 - Accuracy: 0.7216\nEpoch 29/30 - Loss: 0.9675 - Accuracy: 0.6289\nEpoch 30/30 - Loss: 0.8224 - Accuracy: 0.7320\nTraining completed and model saved as trained_imaging_model_dataset2.pth\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!ls ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport cv2\nfrom pathlib import Path\n\n# Utility Function: Get Unique Labels\ndef get_unique_labels(labels_dir):\n    labels_dir = Path(labels_dir)\n    unique_labels = set()\n    for label_file in labels_dir.glob(\"*.txt\"):\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                try:\n                    unique_labels.add(int(line.split()[0]))\n                except Exception as e:\n                    print(f\"Error reading label from {label_file}: {e}\")\n    return unique_labels\n\n# Custom Dataset for Processed Imaging Data (reuse from Dataset2 code)\nclass ProcessedImagingDataset(Dataset):\n    \"\"\"\n    Custom dataset for loading preprocessed images and labels.\n    Filters out images with missing or empty label files.\n    \"\"\"\n    def __init__(self, images_dir, labels_dir, transform=None):\n        self.images_dir = Path(images_dir)\n        self.labels_dir = Path(labels_dir)\n        all_images = list(self.images_dir.glob(\"*.jpg\"))\n        self.image_files = [img for img in all_images \n                            if (self.labels_dir / (img.stem + \".txt\")).exists() \n                            and (self.labels_dir / (img.stem + \".txt\")).stat().st_size > 0]\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        img = cv2.imread(str(img_path))\n        if img is None:\n            raise RuntimeError(f\"Unable to read image: {img_path}\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n        \n        label_file = self.labels_dir / (img_path.stem + \".txt\")\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                label = int(line.split()[0])\n            else:\n                raise ValueError(f\"No label found in {label_file}\")\n        label = torch.tensor(label, dtype=torch.long)\n        # Adjust the expected range based on Dataset3's classes (update if needed)\n        if label.item() < 0:\n            raise ValueError(f\"Label {label.item()} from {label_file} is negative.\")\n        return img, label\n\n# ImagingClassifier using the HybridImagingModel defined earlier.\nclass ImagingClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(ImagingClassifier, self).__init__()\n        # HybridImagingModel is assumed to be defined in a previous cell.\n        self.hybrid_model = HybridImagingModel()\n        self.classifier = nn.Linear(512, num_classes)\n    \n    def forward(self, x):\n        features = self.hybrid_model(x)\n        logits = self.classifier(features)\n        return logits\n\n# Training function remains similar.\ndef train_model(model, dataloader, criterion, optimizer, device, num_epochs=30):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += torch.sum(preds == labels)\n            total += labels.size(0)\n        \n        epoch_loss = running_loss / total\n        epoch_acc = correct.double() / total\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n    return model\n\n# Main Training Loop for Dataset3 Fine-Tuning\nif __name__ == \"__main__\":\n    # Set paths for processed Dataset3.\n    images_dir = \"/kaggle/input/processed-datasets/processed_dataset3/kaggle/working/data/processed/dataset3/train/images\"\n    labels_dir = \"/kaggle/input/processed-datasets/processed_dataset3/kaggle/working/data/processed/dataset3/train/labels\"\n    \n    unique_labels = get_unique_labels(labels_dir)\n    print(\"Unique labels in training set (Dataset3):\", unique_labels)\n    num_classes = len(unique_labels)\n    \n    transform = T.Compose([\n        T.ToPILImage(),\n        T.Resize((640, 640)),\n        T.ToTensor(),\n    ])\n    \n    train_dataset = ProcessedImagingDataset(images_dir, labels_dir, transform=transform)\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n    \n    # Test forward pass on CPU.\n    dummy_img, dummy_label = train_dataset[0]\n    dummy_img = dummy_img.unsqueeze(0)\n    model_cpu = ImagingClassifier(num_classes=num_classes)\n    try:\n        test_out = model_cpu(dummy_img)\n        print(\"Forward pass on CPU successful, output shape:\", test_out.shape)\n    except Exception as e:\n        print(\"Error during forward pass on CPU:\", e)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = ImagingClassifier(num_classes=num_classes).to(device)\n    \n    # Optional: Load pretrained weights from Dataset2 (or Dataset1) for fine-tuning.\n    # For example, using the weights from Dataset2:\n    pretrained_path = \"/kaggle/input/fine-tuning/pytorch/version2/1/trained_imaging_model_dataset2.pth\"\n    if os.path.exists(pretrained_path):\n        state_dict = torch.load(pretrained_path, map_location=device)\n        # Remove classifier keys (since Dataset2 and Dataset3 may have different number of classes)\n        filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"classifier\")}\n        model.load_state_dict(filtered_state_dict, strict=False)\n        print(\"Loaded pretrained weights from Dataset2 (excluding classifier) for fine-tuning on Dataset3.\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n    \n    print(\"Starting fine-tuning training on Dataset3...\")\n    trained_model = train_model(model, train_loader, criterion, optimizer, device, num_epochs=30)\n    \n    torch.save(trained_model.state_dict(), \"trained_imaging_model_dataset3.pth\")\n    print(\"Training completed and model saved as trained_imaging_model_dataset3.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:05.510615Z","iopub.execute_input":"2025-03-22T05:21:05.511017Z","iopub.status.idle":"2025-03-22T05:27:04.123281Z","shell.execute_reply.started":"2025-03-22T05:21:05.510987Z","shell.execute_reply":"2025-03-22T05:27:04.122413Z"}},"outputs":[{"name":"stdout","text":"Unique labels in training set (Dataset3): {0, 1}\nForward pass on CPU successful, output shape: torch.Size([1, 2])\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-917021064006>:144: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(pretrained_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained weights from Dataset2 (excluding classifier) for fine-tuning on Dataset3.\nStarting fine-tuning training on Dataset3...\nEpoch 1/30 - Loss: 0.5285 - Accuracy: 0.7436\nEpoch 2/30 - Loss: 0.3399 - Accuracy: 0.8718\nEpoch 3/30 - Loss: 0.2964 - Accuracy: 0.8718\nEpoch 4/30 - Loss: 0.3162 - Accuracy: 0.8462\nEpoch 5/30 - Loss: 0.2633 - Accuracy: 0.9231\nEpoch 6/30 - Loss: 0.2629 - Accuracy: 0.8462\nEpoch 7/30 - Loss: 0.2307 - Accuracy: 0.8718\nEpoch 8/30 - Loss: 0.2087 - Accuracy: 0.9231\nEpoch 9/30 - Loss: 0.2196 - Accuracy: 0.8718\nEpoch 10/30 - Loss: 0.1789 - Accuracy: 0.9231\nEpoch 11/30 - Loss: 0.1445 - Accuracy: 0.9487\nEpoch 12/30 - Loss: 0.1457 - Accuracy: 0.9231\nEpoch 13/30 - Loss: 0.1640 - Accuracy: 0.9487\nEpoch 14/30 - Loss: 0.1561 - Accuracy: 0.8974\nEpoch 15/30 - Loss: 0.1114 - Accuracy: 0.9487\nEpoch 16/30 - Loss: 0.1598 - Accuracy: 0.9487\nEpoch 17/30 - Loss: 0.1527 - Accuracy: 0.9487\nEpoch 18/30 - Loss: 0.1094 - Accuracy: 0.9487\nEpoch 19/30 - Loss: 0.1167 - Accuracy: 0.9231\nEpoch 20/30 - Loss: 0.2431 - Accuracy: 0.8718\nEpoch 21/30 - Loss: 0.1429 - Accuracy: 0.9231\nEpoch 22/30 - Loss: 0.1504 - Accuracy: 0.9744\nEpoch 23/30 - Loss: 0.1869 - Accuracy: 0.9231\nEpoch 24/30 - Loss: 0.1318 - Accuracy: 0.9231\nEpoch 25/30 - Loss: 0.0649 - Accuracy: 0.9744\nEpoch 26/30 - Loss: 0.1053 - Accuracy: 0.9487\nEpoch 27/30 - Loss: 0.1453 - Accuracy: 0.8974\nEpoch 28/30 - Loss: 0.2097 - Accuracy: 0.8974\nEpoch 29/30 - Loss: 0.0848 - Accuracy: 0.9744\nEpoch 30/30 - Loss: 0.0726 - Accuracy: 0.9744\nTraining completed and model saved as trained_imaging_model_dataset3.pth\n","output_type":"stream"}],"execution_count":8}]}