{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11116238,"sourceType":"datasetVersion","datasetId":6931201}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/input/processed-datasets/processed_dataset1/kaggle/working/data/processed/dataset1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:40:14.111242Z","iopub.execute_input":"2025-03-22T04:40:14.111510Z","iopub.status.idle":"2025-03-22T04:40:14.259783Z","shell.execute_reply.started":"2025-03-22T04:40:14.111462Z","shell.execute_reply":"2025-03-22T04:40:14.258536Z"}},"outputs":[{"name":"stdout","text":"test  train  train_aug\tvalid\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNNBranch(nn.Module):\n    \"\"\"\n    CNN branch to extract local features.\n    Input: (batch, 3, 640, 640)\n    Output: (batch, 128) after global average pooling.\n    \"\"\"\n    def __init__(self):\n        super(CNNBranch, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)  # (32, 320, 320)\n        )\n        self.block2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)  # (64, 160, 160)\n        )\n        self.block3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)  # (128, 80, 80)\n        )\n    \n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        return x.view(x.size(0), -1)\n\n# Quick test:\ndummy_input = torch.randn(4, 3, 640, 640)\ncnn_model = CNNBranch()\nprint(\"CNN Branch Output Shape:\", cnn_model(dummy_input).shape)  # Expected: [4, 128]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:40:52.649182Z","iopub.execute_input":"2025-03-22T04:40:52.649388Z","iopub.status.idle":"2025-03-22T04:40:57.824646Z","shell.execute_reply.started":"2025-03-22T04:40:52.649368Z","shell.execute_reply":"2025-03-22T04:40:57.823743Z"}},"outputs":[{"name":"stdout","text":"CNN Branch Output Shape: torch.Size([4, 128])\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ViTBranch(nn.Module):\n    \"\"\"\n    ViT branch to extract global features.\n    Splits the image into patches, embeds them, adds positional encoding,\n    and processes them with Transformer encoder layers.\n    Input: (batch, 3, 640, 640)\n    Output: (batch, 768)\n    \"\"\"\n    def __init__(self, image_size=640, patch_size=16, in_channels=3, embed_dim=768, num_layers=7, num_heads=8):\n        super(ViTBranch, self).__init__()\n        self.patch_size = patch_size\n        num_patches = (image_size // patch_size) ** 2  # 1600 patches for 640x640\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n        # Use batch_first=True to improve performance and debugging\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=0.1, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n    \n    def forward(self, x):\n        x = self.proj(x)  # (batch, embed_dim, 40, 40)\n        x = x.flatten(2).transpose(1, 2)  # (batch, 1600, embed_dim)\n        x = x + self.pos_embedding\n        x = self.transformer(x)\n        x = x.mean(dim=1)\n        return x\n\n# Quick test:\nvit_model = ViTBranch()\nprint(\"ViT Branch Output Shape:\", vit_model(dummy_input).shape)  # Expected: [4, 768]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:41:05.815314Z","iopub.execute_input":"2025-03-22T04:41:05.815588Z","iopub.status.idle":"2025-03-22T04:41:20.890104Z","shell.execute_reply.started":"2025-03-22T04:41:05.815567Z","shell.execute_reply":"2025-03-22T04:41:20.889191Z"}},"outputs":[{"name":"stdout","text":"ViT Branch Output Shape: torch.Size([4, 768])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass HybridImagingModel(nn.Module):\n    \"\"\"\n    Hybrid Imaging Model combining the CNN and ViT branches.\n    Uses simple concatenation of CNN (128-dim) and ViT (768-dim) outputs,\n    then projects to a 512-dim feature vector.\n    \"\"\"\n    def __init__(self):\n        super(HybridImagingModel, self).__init__()\n        self.cnn_branch = CNNBranch()  # Defined in Cell 1\n        self.vit_branch = ViTBranch()    # Defined in Cell 2\n        self.fc = nn.Linear(128 + 768, 512)\n    \n    def forward(self, x):\n        cnn_features = self.cnn_branch(x)\n        vit_features = self.vit_branch(x)\n        fused_features = torch.cat((cnn_features, vit_features), dim=1)\n        out = self.fc(fused_features)\n        return out\n\n# Quick test:\nhybrid_model = HybridImagingModel()\nprint(\"Hybrid Model Output Shape:\", hybrid_model(dummy_input).shape)  # Expected: [4, 512]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:41:27.764065Z","iopub.execute_input":"2025-03-22T04:41:27.764382Z","iopub.status.idle":"2025-03-22T04:41:42.205579Z","shell.execute_reply.started":"2025-03-22T04:41:27.764353Z","shell.execute_reply":"2025-03-22T04:41:42.204912Z"}},"outputs":[{"name":"stdout","text":"Hybrid Model Output Shape: torch.Size([4, 512])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%env CUDA_LAUNCH_BLOCKING=1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:41:48.777307Z","iopub.execute_input":"2025-03-22T04:41:48.777610Z","iopub.status.idle":"2025-03-22T04:41:48.782338Z","shell.execute_reply.started":"2025-03-22T04:41:48.777574Z","shell.execute_reply":"2025-03-22T04:41:48.781627Z"}},"outputs":[{"name":"stdout","text":"env: CUDA_LAUNCH_BLOCKING=1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport cv2\nfrom pathlib import Path\n\n# --- Define Utility and Dataset classes (reuse from above) ---\n\ndef get_unique_labels(labels_dir):\n    labels_dir = Path(labels_dir)\n    unique_labels = set()\n    for label_file in labels_dir.glob(\"*.txt\"):\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                try:\n                    unique_labels.add(int(line.split()[0]))\n                except Exception as e:\n                    print(f\"Error reading label from {label_file}: {e}\")\n    return unique_labels\n\nclass ProcessedImagingDataset(Dataset):\n    \"\"\"\n    Custom dataset for preprocessed images and labels.\n    Filters out images with missing or empty label files.\n    \"\"\"\n    def __init__(self, images_dir, labels_dir, transform=None):\n        self.images_dir = Path(images_dir)\n        self.labels_dir = Path(labels_dir)\n        all_images = list(self.images_dir.glob(\"*.jpg\"))\n        # Filter images that have a corresponding non-empty label file\n        self.image_files = []\n        for img in all_images:\n            label_file = self.labels_dir / (img.stem + \".txt\")\n            if label_file.exists() and label_file.stat().st_size > 0:\n                self.image_files.append(img)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        img = cv2.imread(str(img_path))\n        if img is None:\n            raise RuntimeError(f\"Unable to read image: {img_path}\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n        \n        label_file = self.labels_dir / (img_path.stem + \".txt\")\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                label = int(line.split()[0])\n            else:\n                raise ValueError(f\"No label found in {label_file}\")\n        \n        label = torch.tensor(label, dtype=torch.long)\n        if label.item() < 0 or label.item() > 8:\n            raise ValueError(f\"Label {label.item()} from {label_file} is out of expected range [0, 8].\")\n        return img, label\n\n# ImagingClassifier definition (using HybridImagingModel from Cell 3)\nclass ImagingClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(ImagingClassifier, self).__init__()\n        self.hybrid_model = HybridImagingModel()  # Defined in Cell 3\n        self.classifier = nn.Linear(512, num_classes)\n    \n    def forward(self, x):\n        features = self.hybrid_model(x)\n        logits = self.classifier(features)\n        return logits\n\ndef train_model(model, dataloader, criterion, optimizer, device, num_epochs=20):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += torch.sum(preds == labels)\n            total += labels.size(0)\n        \n        epoch_loss = running_loss / total\n        epoch_acc = correct.double() / total\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n    return model\n\n# --- Main Training Loop ---\nif __name__ == \"__main__\":\n    # Set paths for Dataset1 processed data.\n    images_dir = \"/kaggle/input/processed-datasets/processed_dataset1/kaggle/working/data/processed/dataset1/train/images\"\n    labels_dir = \"/kaggle/input/processed-datasets/processed_dataset1/kaggle/working/data/processed/dataset1/train/labels\"\n    \n    unique_labels = get_unique_labels(labels_dir)\n    print(\"Unique labels in training set:\", unique_labels)\n    num_classes = len(unique_labels)\n    \n    transform = T.Compose([\n        T.ToPILImage(),\n        T.Resize((640, 640)),\n        T.ToTensor(),\n    ])\n    \n    train_dataset = ProcessedImagingDataset(images_dir, labels_dir, transform=transform)\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n    \n    # Test forward pass on CPU.\n    dummy_img, dummy_label = train_dataset[0]\n    dummy_img = dummy_img.unsqueeze(0)\n    model_cpu = ImagingClassifier(num_classes=num_classes)\n    try:\n        test_out = model_cpu(dummy_img)\n        print(\"Forward pass on CPU successful, output shape:\", test_out.shape)\n    except Exception as e:\n        print(\"Error during forward pass on CPU:\", e)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = ImagingClassifier(num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    \n    print(\"Starting training of the Imaging Module on Dataset1...\")\n    trained_model = train_model(model, train_loader, criterion, optimizer, device, num_epochs=20)\n    \n    torch.save(trained_model.state_dict(), \"trained_imaging_model.pth\")\n    print(\"Training completed and model saved as trained_imaging_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T01:17:50.720459Z","iopub.execute_input":"2025-03-22T01:17:50.720784Z","iopub.status.idle":"2025-03-22T02:07:05.410359Z","shell.execute_reply.started":"2025-03-22T01:17:50.720757Z","shell.execute_reply":"2025-03-22T02:07:05.409241Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell: Training Script for Imaging Module on Dataset2\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport cv2\nfrom pathlib import Path\n\n# --- Utility: Get Unique Labels from the Training Set ---\ndef get_unique_labels(labels_dir):\n    labels_dir = Path(labels_dir)\n    unique_labels = set()\n    for label_file in labels_dir.glob(\"*.txt\"):\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                try:\n                    unique_labels.add(int(line.split()[0]))\n                except Exception as e:\n                    print(f\"Error reading label from {label_file}: {e}\")\n    return unique_labels\n\n# --- Custom Dataset for Processed Imaging Data ---\nclass ProcessedImagingDataset(Dataset):\n    \"\"\"\n    Custom dataset for loading preprocessed images and labels.\n    Filters out images with missing or empty label files.\n    \"\"\"\n    def __init__(self, images_dir, labels_dir, transform=None):\n        self.images_dir = Path(images_dir)\n        self.labels_dir = Path(labels_dir)\n        all_images = list(self.images_dir.glob(\"*.jpg\"))\n        # Filter images that have a corresponding non-empty label file\n        self.image_files = [img for img in all_images \n                            if (self.labels_dir / (img.stem + \".txt\")).exists() \n                            and (self.labels_dir / (img.stem + \".txt\")).stat().st_size > 0]\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        img = cv2.imread(str(img_path))\n        if img is None:\n            raise RuntimeError(f\"Unable to read image: {img_path}\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n        \n        label_file = self.labels_dir / (img_path.stem + \".txt\")\n        with open(label_file, \"r\") as f:\n            line = f.readline().strip()\n            if line:\n                label = int(line.split()[0])\n            else:\n                raise ValueError(f\"No label found in {label_file}\")\n        label = torch.tensor(label, dtype=torch.long)\n        if label.item() < 0:\n            raise ValueError(f\"Label {label.item()} from {label_file} is negative.\")\n        return img, label\n\n# --- ImagingClassifier Definition ---\n# Assumes HybridImagingModel is already defined (from Cell 3)\nclass ImagingClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(ImagingClassifier, self).__init__()\n        self.hybrid_model = HybridImagingModel()  # Defined in earlier cell\n        self.classifier = nn.Linear(512, num_classes)\n    \n    def forward(self, x):\n        features = self.hybrid_model(x)\n        logits = self.classifier(features)\n        return logits\n\n# --- Training Function ---\ndef train_model(model, dataloader, criterion, optimizer, device, num_epochs=20):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += torch.sum(preds == labels)\n            total += labels.size(0)\n        \n        epoch_loss = running_loss / total\n        epoch_acc = correct.double() / total\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}\")\n    return model\n\n# --- Main Training Loop for Dataset2 ---\nif __name__ == \"__main__\":\n    # Set paths for processed Dataset2 (update if needed)\n    images_dir = \"/kaggle/input/processed-datasets/processed_dataset2/kaggle/working/data/processed/dataset2/train/images\"\n    labels_dir = \"/kaggle/input/processed-datasets/processed_dataset2/kaggle/working/data/processed/dataset2/train/labels\"\n    \n    unique_labels = get_unique_labels(labels_dir)\n    print(\"Unique labels in training set (Dataset2):\", unique_labels)\n    num_classes = len(unique_labels)\n    \n    transform = T.Compose([\n        T.ToPILImage(),\n        T.Resize((640, 640)),\n        T.ToTensor(),\n    ])\n    \n    train_dataset = ProcessedImagingDataset(images_dir, labels_dir, transform=transform)\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n    \n    # Test a forward pass on CPU\n    dummy_img, dummy_label = train_dataset[0]\n    dummy_img = dummy_img.unsqueeze(0)\n    model_cpu = ImagingClassifier(num_classes=num_classes)\n    try:\n        test_out = model_cpu(dummy_img)\n        print(\"Forward pass on CPU successful, output shape:\", test_out.shape)\n    except Exception as e:\n        print(\"Error during forward pass on CPU:\", e)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = ImagingClassifier(num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    \n    print(\"Starting training of the Imaging Module on Dataset2...\")\n    trained_model = train_model(model, train_loader, criterion, optimizer, device, num_epochs=20)\n    \n    torch.save(trained_model.state_dict(), \"trained_imaging_model_dataset2.pth\")\n    print(\"Training completed and model saved as trained_imaging_model_dataset2.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T04:42:06.219752Z","iopub.execute_input":"2025-03-22T04:42:06.220074Z","iopub.status.idle":"2025-03-22T04:51:42.349303Z","shell.execute_reply.started":"2025-03-22T04:42:06.220053Z","shell.execute_reply":"2025-03-22T04:51:42.348153Z"}},"outputs":[{"name":"stdout","text":"Unique labels in training set (Dataset2): {0, 1, 2, 3, 4}\nForward pass on CPU successful, output shape: torch.Size([1, 5])\nStarting training of the Imaging Module on Dataset2...\nEpoch 1/20 - Loss: 3.0570 - Accuracy: 0.2474\nEpoch 2/20 - Loss: 1.5775 - Accuracy: 0.3299\nEpoch 3/20 - Loss: 1.4627 - Accuracy: 0.3711\nEpoch 4/20 - Loss: 1.5724 - Accuracy: 0.4021\nEpoch 5/20 - Loss: 1.7034 - Accuracy: 0.3093\nEpoch 6/20 - Loss: 1.2537 - Accuracy: 0.4845\nEpoch 7/20 - Loss: 1.3138 - Accuracy: 0.4124\nEpoch 8/20 - Loss: 1.5740 - Accuracy: 0.4433\nEpoch 9/20 - Loss: 1.6407 - Accuracy: 0.3918\nEpoch 10/20 - Loss: 1.3514 - Accuracy: 0.4639\nEpoch 11/20 - Loss: 1.4251 - Accuracy: 0.4536\nEpoch 12/20 - Loss: 1.2154 - Accuracy: 0.5052\nEpoch 13/20 - Loss: 1.1211 - Accuracy: 0.6186\nEpoch 14/20 - Loss: 1.1916 - Accuracy: 0.4948\nEpoch 15/20 - Loss: 1.1331 - Accuracy: 0.5258\nEpoch 16/20 - Loss: 1.2061 - Accuracy: 0.4845\nEpoch 17/20 - Loss: 1.1286 - Accuracy: 0.5567\nEpoch 18/20 - Loss: 1.2185 - Accuracy: 0.4845\nEpoch 19/20 - Loss: 1.0524 - Accuracy: 0.5876\nEpoch 20/20 - Loss: 1.0506 - Accuracy: 0.5979\nTraining completed and model saved as trained_imaging_model_dataset2.pth\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!ls ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}